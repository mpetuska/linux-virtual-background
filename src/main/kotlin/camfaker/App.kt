/*
 * This Kotlin source file was generated by the Gradle 'init' task.
 */
package camfaker

import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel
import java.nio.file.Files
import java.nio.file.Paths
import java.nio.file.StandardOpenOption
import java.util.EnumSet

object App {
  val greeting: String
    get() {
      return "Hello World!"
    }

  fun getResourceByteBuffer(name: String): MappedByteBuffer {
    val path = javaClass.classLoader.getResource(name)?.path?.let(Paths::get)!!
    return Files.newByteChannel(path, EnumSet.of(StandardOpenOption.READ)).use {
      (it as FileChannel).map(FileChannel.MapMode.READ_ONLY, 0, it.size())
    }
  }
}

fun main(vararg args: String) {
  println(App.greeting)
  startCamera(*args)
  // var classifierName: String?
  // if (args.size > 0) {
  //   classifierName = args[0]
  // } else {
  //   val url = URL("https://raw.github.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml")
  //   val file: File = Loader.cacheResource(url)
  //   classifierName = file.absolutePath
  // }
  //
  // // We can "cast" Pointer objects by instantiating a new object of the desired class.
  //
  // // We can "cast" Pointer objects by instantiating a new object of the desired class.
  // val classifier = CascadeClassifier(classifierName)
  // if (classifier == null) {
  //   System.err.println("Error loading classifier file \"$classifierName\".")
  //   System.exit(1)
  // }
  // val grabber = FrameGrabber.createDefault("/dev/video0")
  // grabber.start()
  // val converter = OpenCVFrameConverter.ToMat()
  // var grabbedImage: Mat = converter.convert(grabber.grab())
  // val height = grabbedImage.rows()
  // val width = grabbedImage.cols()
  // val grayImage = Mat(height, width, CV_8UC1)
  // val rotatedImage: Mat = grabbedImage.clone()
  //
  // // CanvasFrame is a JFrame containing a Canvas component, which is hardware accelerated.
  // // It can also switch into full-screen mode when called with a screenNumber.
  // // We should also specify the relative monitor/camera response for proper gamma correction.
  // // CanvasFrame is a JFrame containing a Canvas component, which is hardware accelerated.
  // // It can also switch into full-screen mode when called with a screenNumber.
  // // We should also specify the relative monitor/camera response for proper gamma correction.
  // val frame = CanvasFrame("Some Title", CanvasFrame.getDefaultGamma() / grabber.gamma)
  //
  // // Let's create some random 3D rotation...
  //
  // // Let's create some random 3D rotation...
  // val randomR = Mat(3, 3, CV_64FC1)
  // val randomAxis = Mat(3, 1, CV_64FC1)
  // // We can easily and efficiently access the elements of matrices and images
  // // through an Indexer object with the set of get() and put() methods.
  // // We can easily and efficiently access the elements of matrices and images
  // // through an Indexer object with the set of get() and put() methods.
  // val Ridx = randomR.createIndexer<DoubleIndexer>()
  // val axisIdx = randomAxis.createIndexer<DoubleIndexer>()
  // axisIdx.put(
  //   0, (Math.random() - 0.5) / 4,
  //   (Math.random() - 0.5) / 4,
  //   (Math.random() - 0.5) / 4
  // )
  // Rodrigues(randomAxis, randomR)
  // val f: Double = (width + height) / 2.0
  // Ridx.put(0, 2, Ridx.get(0, 2) * f)
  // Ridx.put(1, 2, Ridx[1, 2] * f)
  // Ridx.put(2, 0, Ridx[2, 0] / f)
  // Ridx.put(2, 1, Ridx.get(2, 1) / f)
  // println(Ridx)
  //
  // // We can allocate native arrays using constructors taking an integer as argument.
  //
  // // We can allocate native arrays using constructors taking an integer as argument.
  // val hatPoints = Point(3)
  // val videoDevice = File("/dev/video20")
  // while (frame.isVisible && converter.convert(grabber.grab()).also { grabbedImage = it } != null) {
  //   // Let's try to detect some faces! but we need a grayscale image...
  //   cvtColor(grabbedImage, grayImage, COLOR_RGB2YUV)
  //   val faces = RectVector()
  //   classifier.detectMultiScale(grayImage, faces)
  //   val total = faces.size()
  //   for (i in 0 until total) {
  //     val r: Rect = faces[i]
  //     val x: Int = r.x()
  //     val y: Int = r.y()
  //     val w: Int = r.width()
  //     val h: Int = r.height()
  //     rectangle(grabbedImage, Point(x, y), Point(x + w, y + h), Scalar.RED, 1, CV_AA, 0)
  //
  //     // To access or pass as argument the elements of a native array, call position() before.
  //     hatPoints.position(0).x(x - w / 10).y(y - h / 10)
  //     hatPoints.position(1).x(x + w * 11 / 10).y(y - h / 10)
  //     hatPoints.position(2).x(x + w / 2).y(y - h / 2)
  //     fillConvexPoly(grabbedImage, hatPoints.position(0), 3, Scalar.GREEN, CV_AA, 0)
  //   }
  //
  //   // Let's find some contours! but first some thresholding...
  //   threshold(grayImage, grayImage, 64.0, 255.0, CV_THRESH_BINARY)
  //
  //   // To check if an output argument is null we may call either isNull() or equals(null).
  //   val contours = MatVector()
  //   findContours(grayImage, contours, CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE)
  //   val n = contours.size()
  //   for (i in 0 until n) {
  //     val contour = contours[i]
  //     val points = Mat()
  //     approxPolyDP(contour, points, arcLength(contour, true) * 0.02, true)
  //     drawContours(grabbedImage, MatVector(points), -1, Scalar.BLUE)
  //   }
  //   warpPerspective(grabbedImage, rotatedImage, randomR, rotatedImage.size())
  //   val rotatedFrame: Frame = converter.convert(rotatedImage)
  //   frame.showImage(rotatedFrame)
  //
  //   videoDevice.writeText(rotatedFrame.image.toString())
  // }
  // frame.dispose()
  // grabber.stop()
  // val model = App.getResourceByteBuffer("models/bodypix_mobilenet_v1_075_480_352_16_quant_edgetpu_decoder.tflite")
//  Interpreter(model)
}
